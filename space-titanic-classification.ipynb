{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robertmorsch/space-titanic-eda-cv-hyperopt-automl?scriptVersionId=122037845\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Hello and welcome to my Space Titanic effort to predict which passengers are transported to an alternate dimension.\nThis is my analysis of the Kaggle dataset \"spaceship-titanic\"\n\nNOTE FOR BEGINNERS: this notebook is merely the final notebook of my analytic work for this competition. I recommend, if you are a beginner, to approach your work similar to writing a draft in school. Begin by dumping your thoughts outs, and iterate by addings structure and removing junk. Don't let yourself get stuck trying to write eloquently, automating, cleaning, or engineering features, on your first go around.","metadata":{"execution":{"iopub.status.busy":"2023-03-11T21:12:57.192344Z","iopub.execute_input":"2023-03-11T21:12:57.19315Z","iopub.status.idle":"2023-03-11T21:12:57.220906Z","shell.execute_reply.started":"2023-03-11T21:12:57.193022Z","shell.execute_reply":"2023-03-11T21:12:57.219884Z"}}},{"cell_type":"code","source":"#Importing packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-13T15:50:11.339507Z","iopub.execute_input":"2023-03-13T15:50:11.340931Z","iopub.status.idle":"2023-03-13T15:50:12.941001Z","shell.execute_reply.started":"2023-03-13T15:50:11.340784Z","shell.execute_reply":"2023-03-13T15:50:12.939481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pulling in the data\nsubmission = pd.read_csv(\"/kaggle/input/spaceship-titanic/sample_submission.csv\")\npassengers = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ntestset_passengers = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:12.943196Z","iopub.execute_input":"2023-03-13T15:50:12.944559Z","iopub.status.idle":"2023-03-13T15:50:13.093471Z","shell.execute_reply.started":"2023-03-13T15:50:12.944492Z","shell.execute_reply":"2023-03-13T15:50:13.092085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the dataset as a whole","metadata":{}},{"cell_type":"code","source":"#Getting to know the data.\n#Its rows vs columns\nprint(\"Shape: \",passengers.shape)\nprint(\"\\n\")\n#Rows containing missing data by column\nprint(\"Nulls by Col: \",\"\\n\")\nprint(passengers.isnull().sum().sort_values())\nprint(\"\\n\")\n#Total missing data points\nprint(\"Total Nulls: \",passengers.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:13.095809Z","iopub.execute_input":"2023-03-13T15:50:13.096912Z","iopub.status.idle":"2023-03-13T15:50:13.1286Z","shell.execute_reply.started":"2023-03-13T15:50:13.096858Z","shell.execute_reply":"2023-03-13T15:50:13.127288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decent amount of rows, far from big data. 14 Columns is fairly rich and give some decent options, however Transported is the TARGET variable, and PassengerID will pointless due to its high cardinality.","metadata":{"execution":{"iopub.status.busy":"2023-03-11T21:14:23.6415Z","iopub.execute_input":"2023-03-11T21:14:23.64203Z","iopub.status.idle":"2023-03-11T21:14:23.648202Z","shell.execute_reply.started":"2023-03-11T21:14:23.64199Z","shell.execute_reply":"2023-03-11T21:14:23.646722Z"}}},{"cell_type":"code","source":"#Columns and their datatype\n#DataFrame 5 row sample\npassengers.info()\npassengers.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:13.132554Z","iopub.execute_input":"2023-03-13T15:50:13.133721Z","iopub.status.idle":"2023-03-13T15:50:13.212463Z","shell.execute_reply.started":"2023-03-13T15:50:13.133666Z","shell.execute_reply":"2023-03-13T15:50:13.211146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In previous interations, I explored and confirmed the following assumptions. But from what we can see from the sample, the attention grabbing features are that we can transform upfront.\n* True/False columns that should be transformed into 1/0: CryoSleep, VIP, and Transported. \n* Cabin is a compound column that should be split into 3 separate columns: deck, num, and side.\n* Name into 2 columns: first_name and last_name.\n    * The primary reason for this is that we can create and easy new column from this by calculating family size aboard. \n    * Names are rarely useful in themselves due to their high cardinality.","metadata":{}},{"cell_type":"markdown","source":"This function is meant to apply the changes I identified in the previous text block. The reason it is a function is so I can reuse it for the TEST DATASET. This is the reason for the IF, the TEST DATASET does not contain the Transported column.","metadata":{}},{"cell_type":"code","source":"def earlyTransformations(dataset, is_training_set = True):\n    #Will likely not be used - Dropping PassengerID\n    if is_training_set:\n        dataset[['Transported']] = dataset[['Transported']]*1\n    \n    #Transforming True/False to 1/0\n    dataset[['CryoSleep','VIP']] = dataset[['CryoSleep','VIP']]*1\n    \n    #Splitting Cabin into deck/num/side columns\n    #Dropping the Cabin column\n    dataset[['deck','num','side']] = dataset['Cabin'].str.split(pat='/',expand=True)\n    dataset = dataset.drop('Cabin',axis=1)\n    dataset['num'] = dataset['num'].astype('float64')\n\n    #Spliting Name into First Name and Last Name\n    #Creating Family Size column from number of occurences of a passenger's last name\n    dataset[['first_name', 'last_name']] = dataset['Name'].str.split(expand=True)\n    dataset['family_size'] = dataset.groupby('last_name')['last_name'].transform('count')\n    \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:13.214401Z","iopub.execute_input":"2023-03-13T15:50:13.215197Z","iopub.status.idle":"2023-03-13T15:50:13.226958Z","shell.execute_reply.started":"2023-03-13T15:50:13.215145Z","shell.execute_reply":"2023-03-13T15:50:13.225234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observing the changes made to the dataset","metadata":{}},{"cell_type":"code","source":"passengers = earlyTransformations(passengers)\npassengers.info()\npassengers.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:13.229137Z","iopub.execute_input":"2023-03-13T15:50:13.23005Z","iopub.status.idle":"2023-03-13T15:50:13.372527Z","shell.execute_reply.started":"2023-03-13T15:50:13.23Z","shell.execute_reply":"2023-03-13T15:50:13.370942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the missing data per column. This is an extremely important even if it may appear to be redundant. Unfortunately, manipulating columns can often result in, missing data no longer registering as missing. Although, I am not entirely sure of the implications of missing data as not registering as missing, but it is best likely best avoided.","metadata":{}},{"cell_type":"code","source":"passengers.isnull().sum().sort_values()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:13.374444Z","iopub.execute_input":"2023-03-13T15:50:13.37529Z","iopub.status.idle":"2023-03-13T15:50:13.396187Z","shell.execute_reply.started":"2023-03-13T15:50:13.375243Z","shell.execute_reply":"2023-03-13T15:50:13.394633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"Exploring the data, starting with categorical features. Primarily, we are trying to get to know the data for key features or actionable engineering. For example most of the tranformations performed earlier in the notebook were discovered in a previous iteration during EDA.","metadata":{}},{"cell_type":"markdown","source":"# Exploring Categorical Features","metadata":{}},{"cell_type":"markdown","source":"Following code is an iterative function to output visualzations with 2 columns.\n* LEFT column: show the distribution across the categories in each column. \n* RIGHT column: split of the column categories by TARGET value.","metadata":{}},{"cell_type":"code","source":"# Function to display distribution of Categorical columns\n# Will display 2 visuals per column\n# Will need to have columns chosen before hand.\ndef cat_cols_display(df):\n    \n    plt.figure(figsize=(15, 20))\n    plt.subplots_adjust(hspace=0.5)\n    plt.suptitle(\"Cat Feats\", fontsize=18, y=0.95)\n\n    cat_features = df.columns\n    number_of_columns = len(cat_features)\n    n=1\n    \n    # loop through the length of tickers and keep track of index\n    for cat in cat_features:\n        # add a new subplot iteratively\n        ax = plt.subplot(number_of_columns, 2,n)\n\n        # filter df and plot ticker on the new subplot axis\n        sns.countplot(x=passengers[cat],\n                     order = passengers[cat].value_counts().index)\n\n        # chart formatting\n        ax.set_title(cat.upper())\n        #ax.get_legend().remove()\n        ax.set_xlabel(\"\")\n        ax.bar_label(ax.containers[0])\n\n        n +=1 #It\n\n        ax2 = plt.subplot(number_of_columns, 2,n)\n        sns.countplot(x=passengers[cat],\n                  order = passengers[cat].value_counts().index,\n                 hue =passengers['Transported'])\n\n        n +=1\n\n        # chart formatting\n        ax2.set_title(cat.upper())\n        #ax.get_legend().remove()\n        ax2.set_xlabel(\"\")\n        ax2.bar_label(ax2.containers[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:13.398161Z","iopub.execute_input":"2023-03-13T15:50:13.398683Z","iopub.status.idle":"2023-03-13T15:50:13.414377Z","shell.execute_reply.started":"2023-03-13T15:50:13.398636Z","shell.execute_reply":"2023-03-13T15:50:13.412431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking which columns are categorical\n#Calling the function to display exploratory visualizations of categorical columns\nprint(passengers.select_dtypes(include='object').columns)\ncat_cols_display(passengers[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'deck', 'side']])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:13.416129Z","iopub.execute_input":"2023-03-13T15:50:13.416584Z","iopub.status.idle":"2023-03-13T15:50:15.577901Z","shell.execute_reply.started":"2023-03-13T15:50:13.416546Z","shell.execute_reply":"2023-03-13T15:50:15.576513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes on categorical visualizations\n* There are a few columns with sizeable discrpencies. Discrepencies being imbalance in TARGET value in a category within a column.\n* CyroSleep, HomePlanet, and deck having the largest discrepencies, so we should expect these to be the most important for predicting Transported.","metadata":{}},{"cell_type":"markdown","source":"# Exploring Continuous Features","metadata":{}},{"cell_type":"markdown","source":"Similar visualization function as the one for categorical features.\nIterative function to output visualzations with 2 columns.\n* LEFT column: distribution of numbers contained in the feature, organized into bins. \n* RIGHT column: violin plot, split between non transported vs transported.","metadata":{}},{"cell_type":"code","source":"# Function to display distribution of Continuous columns\n# Will display 2 visuals per column\n# Will need to have columns chosen before hand.\n\ndef continuous_cols_display(df):\n    \n    plt.figure(figsize=(15, 40))\n    plt.subplots_adjust(hspace=0.5)\n    plt.suptitle(\"Continuous Feats\", fontsize=18, y=0.95)\n\n    continuous_features = df.columns\n    subplot_row = len(continuous_features)\n    n=1\n    \n    # loop through the length of tickers and keep track of index\n    for feat in continuous_features:\n        # add a new subplot iteratively\n        ax = plt.subplot(subplot_row, 2,n)\n\n        # filter df and plot ticker on the new subplot axis\n        sns.histplot(data=passengers,\n                     x=feat,\n                    bins=50)\n\n        # chart formatting\n        ax.set_title(feat.upper())\n        #ax.get_legend().remove()\n        ax.set_xlabel(\"\")\n        n +=1 #Iterate subplot location\n\n        ax2 = plt.subplot(subplot_row, 2,n)\n        sns.violinplot(data=passengers, \n                       x=\"Transported\", \n                       y=feat)\n\n        n +=1 #Iterate subplot location\n\n        # chart formatting\n        ax2.set_title(feat.upper())\n        ax2.set_xlabel(\"Transported\")","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:15.58362Z","iopub.execute_input":"2023-03-13T15:50:15.584243Z","iopub.status.idle":"2023-03-13T15:50:15.593625Z","shell.execute_reply.started":"2023-03-13T15:50:15.584205Z","shell.execute_reply":"2023-03-13T15:50:15.592454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick view of the descriptive statistics of the coninuous features.","metadata":{}},{"cell_type":"code","source":"passengers.describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:15.595747Z","iopub.execute_input":"2023-03-13T15:50:15.596701Z","iopub.status.idle":"2023-03-13T15:50:15.672443Z","shell.execute_reply.started":"2023-03-13T15:50:15.596659Z","shell.execute_reply":"2023-03-13T15:50:15.670163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(passengers.select_dtypes(exclude='object').columns)\ndf = passengers[['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'num', 'family_size']]\ncontinuous_cols_display(df)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:15.673815Z","iopub.execute_input":"2023-03-13T15:50:15.674587Z","iopub.status.idle":"2023-03-13T15:50:19.201052Z","shell.execute_reply.started":"2023-03-13T15:50:15.674551Z","shell.execute_reply":"2023-03-13T15:50:19.199404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes on categorical visualizations\n* the spending features: RoomService, FoodCourt, ShoppingMall, Spa, and VRDeck have huge skews in their distributions between the target feature.\n* Due to their similar nature (spending money), I will be adding 2 columns to combine these columns: Spending_Total and a binary Spent_Money.\n* The other distributions have some discrepency but not nearly as much. \n\nIn conclusion, the violin plots for the most part are almost mirror images, except for the spending features, which have some large discrepencies. Most notably: VRDeck, Spa, and RoomService.","metadata":{}},{"cell_type":"code","source":"corr_matrix=passengers.corr()\ncorr_matrix['Transported'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.202825Z","iopub.execute_input":"2023-03-13T15:50:19.203267Z","iopub.status.idle":"2023-03-13T15:50:19.218334Z","shell.execute_reply.started":"2023-03-13T15:50:19.203231Z","shell.execute_reply":"2023-03-13T15:50:19.216775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation confirms the large discrepencies we saw in the violin plots, where VRDeck, Spa, and RoomService have the strongest relationship.","metadata":{}},{"cell_type":"markdown","source":"# Data Processing Pipeline","metadata":{}},{"cell_type":"markdown","source":"We are now ready to clean the data. I am doing this through a pipeline. Creating a pipeline is not necessary, but will allow the following code block to be used in future analyses.\nQuick notes on whats happening in pipeline:\n* There will be a split in inputs, continuous features and categorical features,leading to a combined output at the end of the pipeline.\n* SimpleImputer is meant to replace missing values. Here I am using the median for continuous and catch-all 'missing' for categorical.\n* StandardScaler to normalize the continuous features to have the same mean and standard deviation. Allowing for the analysis of continuous features on the same scale.\n* OneHotEncoder generates a new column for each category in every categrical coloumn. Otherwise, the categorical columns are inefficient or impossible to analysis in most ML algorithms.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n#Numeric Pipleine Steps\nnumerical_imputer = SimpleImputer(strategy='median')\nnumerical_scaler = StandardScaler()\n    \n#Categorical Pipleine Steps   \ncategorical_imputer = SimpleImputer(strategy='constant',\n                                    fill_value='missing')\ncategorical_missing_encoder = OneHotEncoder(handle_unknown='ignore')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.219953Z","iopub.execute_input":"2023-03-13T15:50:19.220425Z","iopub.status.idle":"2023-03-13T15:50:19.531045Z","shell.execute_reply.started":"2023-03-13T15:50:19.220375Z","shell.execute_reply":"2023-03-13T15:50:19.529715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Within the pipeline I will be adding 2 new continous columns using the class below.\n* spent_total is the sum of all money spending columns\n* spent_money is binary, 1 for if spent_total is > 0 and 0 for if spent_total is <=0 ","metadata":{}},{"cell_type":"code","source":"#Class to create 2 new columns step in pipeline\n\n#Dynamically finding the columns needed in the \ncol_names = 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'\nRoomService_ix, FoodCourt_ix, ShoppingMall_ix, Spa_ix, VRDeck_ix = [\n    passengers.columns.get_loc(c) for c in col_names] # get the column indices\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass create_columns_for_spending(BaseEstimator, TransformerMixin):\n    '''select specific columns of a given dataset'''\n    def __init__(self):\n        self\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        spent_total = X[:, RoomService_ix] + X[:, FoodCourt_ix] +  X[:, ShoppingMall_ix] + X[:, Spa_ix] +  X[:, VRDeck_ix]\n        spent_money = np.where(spent_total > 0, 1, 0)       \n        return np.c_[X, spent_total, spent_money]\n\n#attr_adder = create_columns_for_spending()\n#passengers_extra_attribs = attr_adder.transform(passengers.values)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.532965Z","iopub.execute_input":"2023-03-13T15:50:19.533398Z","iopub.status.idle":"2023-03-13T15:50:19.544484Z","shell.execute_reply.started":"2023-03-13T15:50:19.533326Z","shell.execute_reply":"2023-03-13T15:50:19.543297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick view of the info and head of the dataset before inset into the pipeline","metadata":{}},{"cell_type":"code","source":"passengers.info()\npassengers.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.545883Z","iopub.execute_input":"2023-03-13T15:50:19.546689Z","iopub.status.idle":"2023-03-13T15:50:19.594895Z","shell.execute_reply.started":"2023-03-13T15:50:19.54665Z","shell.execute_reply":"2023-03-13T15:50:19.593872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identifying the continuous and categorical features. Creating continuous pipeline and categorical pipeline. Then combining these two pipelines into the full pipeline.","metadata":{}},{"cell_type":"code","source":"#Pipeline builds\n\n#Identify and split the column names into numerical or categorical columns\nnumerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'\n                      ,'num', 'family_size', 'CryoSleep', 'VIP']\ncategorical_features = ['HomePlanet', 'Destination', 'deck', 'side']\n\n\nnumerical_pipeline = Pipeline([\n    ('num_imputer',numerical_imputer),\n    ('scaler',numerical_scaler)\n])\n\ncategorical_pipeline = Pipeline([\n    ('cat_imputer',categorical_imputer),\n    ('categorical_missing_encoder',categorical_missing_encoder)\n])\n     \nfull_pipeline = ColumnTransformer([\n    ('num',numerical_pipeline,numerical_features),\n    ('cat',categorical_pipeline,categorical_features)\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.596289Z","iopub.execute_input":"2023-03-13T15:50:19.596716Z","iopub.status.idle":"2023-03-13T15:50:19.605158Z","shell.execute_reply.started":"2023-03-13T15:50:19.59668Z","shell.execute_reply":"2023-03-13T15:50:19.603963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"passengers_labels = passengers['Transported'].copy()\npassengers_prepared = full_pipeline.fit_transform(passengers)\npassengers_prepared","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.60719Z","iopub.execute_input":"2023-03-13T15:50:19.607651Z","iopub.status.idle":"2023-03-13T15:50:19.694055Z","shell.execute_reply.started":"2023-03-13T15:50:19.607612Z","shell.execute_reply":"2023-03-13T15:50:19.692766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Output of the pipeline is a numpy array","metadata":{}},{"cell_type":"markdown","source":"# Cross Validation and Modeling","metadata":{}},{"cell_type":"markdown","source":"Now the fun part. We are ready to train and validate our models.","metadata":{}},{"cell_type":"markdown","source":"Function to run basic cv predictions, to get a baseline comparison of the models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ndef CrossValidate_Model(model,X,y):\n    # Multiply by -1 since sklearn calculates *negative* MAE\n    scores = cross_val_score(model, X, y,\n                                  cv=5,\n                                  scoring='accuracy')\n\n    print(model)\n    print(\"Accuracy scores:\\n\", scores)\n    print(\"Accuracy Average:\\n\", np.average(scores))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.695452Z","iopub.execute_input":"2023-03-13T15:50:19.695808Z","iopub.status.idle":"2023-03-13T15:50:19.702949Z","shell.execute_reply.started":"2023-03-13T15:50:19.695769Z","shell.execute_reply":"2023-03-13T15:50:19.701409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ndef CrossValidate_Model(model,X,y):\n    # Multiply by -1 since sklearn calculates *negative* MAE\n    scores = cross_val_score(model, X, y,\n                                  cv=5,\n                                  scoring='accuracy')\n\n    print(model)\n    print(\"Accuracy scores:\\n\", scores)\n    print(\"Accuracy Average:\\n\", np.average(scores))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.704636Z","iopub.execute_input":"2023-03-13T15:50:19.705105Z","iopub.status.idle":"2023-03-13T15:50:19.71582Z","shell.execute_reply.started":"2023-03-13T15:50:19.705058Z","shell.execute_reply":"2023-03-13T15:50:19.714687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef HyperParam_CV(model, X, y, param_grid):\n        grid_search = GridSearchCV(model,\n                                  param_grid,\n                                  cv=5,\n                                  scoring='accuracy',\n                                  return_train_score=True)\n        grid_search.fit(X, y)\n        print(grid_search.best_params_)\n        print(grid_search.best_score_)\n        return grid_search.best_estimator_\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.717401Z","iopub.execute_input":"2023-03-13T15:50:19.717779Z","iopub.status.idle":"2023-03-13T15:50:19.7316Z","shell.execute_reply.started":"2023-03-13T15:50:19.717747Z","shell.execute_reply":"2023-03-13T15:50:19.72874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to cross validate a given model by accuracy and to apply a Grid Search on a range of hyperparameters for a given model.\nThe function returns each distinct model with it's best performing combination of hyperparameters.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef HyperParam_CV(model, X, y, param_grid):\n        grid_search = GridSearchCV(model,\n                                  param_grid,\n                                  cv=5,\n                                  scoring='accuracy',\n                                  return_train_score=True)\n        grid_search.fit(X, y)\n        print(grid_search.best_params_)\n        print(grid_search.best_score_)\n        return grid_search.best_estimator_\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.733079Z","iopub.execute_input":"2023-03-13T15:50:19.733833Z","iopub.status.idle":"2023-03-13T15:50:19.749209Z","shell.execute_reply.started":"2023-03-13T15:50:19.733792Z","shell.execute_reply":"2023-03-13T15:50:19.748039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating 3 classification models, as well as number of hyperparameters with various potential settings. Here I am running a basic cross validation before running the CV with hyperparameter optimization.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\ndtc = DecisionTreeClassifier(random_state=0)\ndtc_param_grid = [{\n    'min_samples_split': [1,2,5],\n    'max_features': ['auto','sqrt']\n}]\n\nrfc = RandomForestClassifier(random_state=0)\nrfc_param_grid = [{\n    'n_estimators': [100,50,200],\n    'max_features': [None,'auto','sqrt']\n}]\n\nxgb = XGBClassifier()\nxgb_param_grid = [{\n    \"learning_rate\":[0.3, 0.1, 0.03],\n    \"n_estimators\":[100,50,200]\n}]\n\nmodels = [dtc,\n          rfc,\n          xgb]\n\nparam_grids = [dtc_param_grid, \n               rfc_param_grid, \n               xgb_param_grid]\n\nfor model in models:\n    CrossValidate_Model(model,passengers_prepared,passengers_labels)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:19.75075Z","iopub.execute_input":"2023-03-13T15:50:19.75157Z","iopub.status.idle":"2023-03-13T15:50:30.918092Z","shell.execute_reply.started":"2023-03-13T15:50:19.751534Z","shell.execute_reply":"2023-03-13T15:50:30.91717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RandomForestClassifier has the best average accuracy across the 5 folds in cross validation","metadata":{}},{"cell_type":"code","source":"dtc_final_model = HyperParam_CV(models[0], passengers_prepared, passengers_labels, param_grids[0])    ","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:30.919651Z","iopub.execute_input":"2023-03-13T15:50:30.920345Z","iopub.status.idle":"2023-03-13T15:50:31.556771Z","shell.execute_reply.started":"2023-03-13T15:50:30.920305Z","shell.execute_reply":"2023-03-13T15:50:31.555516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{'max_features': None, 'min_samples_split': 10}\n\n0.7302487243505287","metadata":{}},{"cell_type":"code","source":"rfc_final_model = HyperParam_CV(models[1], passengers_prepared, passengers_labels, param_grids[1])  ","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:50:31.558617Z","iopub.execute_input":"2023-03-13T15:50:31.559388Z","iopub.status.idle":"2023-03-13T15:52:18.951202Z","shell.execute_reply.started":"2023-03-13T15:50:31.559319Z","shell.execute_reply":"2023-03-13T15:52:18.950119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{'bootstrap': True, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n\n0.797657476784867","metadata":{}},{"cell_type":"code","source":"xgb_final_model = HyperParam_CV(models[2], passengers_prepared, passengers_labels, param_grids[2])    ","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:52:18.952557Z","iopub.execute_input":"2023-03-13T15:52:18.953229Z","iopub.status.idle":"2023-03-13T15:53:18.798342Z","shell.execute_reply.started":"2023-03-13T15:52:18.953186Z","shell.execute_reply":"2023-03-13T15:53:18.797425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.75}\n\n0.795700014094843","metadata":{}},{"cell_type":"markdown","source":"# LazyPredict","metadata":{}},{"cell_type":"markdown","source":"When in Rome do as the Roman, well when Kaggling do as the Kagglers. Meaning do anything, no matter how impractical within data science's standard and norms, as long as it improves your overall score. On a more serious note, there's a lot of buzz around AutoML. After looking into its implementation with Python, I was pleasantly surprised to learn its actually a lot easier than I expected. Below is me using the library LazyPredict, which is an AutoML library. In this case I am use LazyPredict to only evaluate models a far wider range of model than I am proficient in.","metadata":{}},{"cell_type":"code","source":"pip install lazypredict","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:53:18.800032Z","iopub.execute_input":"2023-03-13T15:53:18.800743Z","iopub.status.idle":"2023-03-13T15:53:32.91904Z","shell.execute_reply.started":"2023-03-13T15:53:18.800704Z","shell.execute_reply":"2023-03-13T15:53:32.917588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, LazyPredict doesn't play well with CrossValidation, so I performed the standard train test split, but running the model is a cakewalk afterwards.","metadata":{}},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(passengers_prepared, \n                                                    passengers_labels, \n                                                    test_size=.33, \n                                                    random_state=42)\n\nclf = LazyClassifier(predictions=True)\nlazy_models, lazy_predictions = clf.fit(X_train, X_test, y_train, y_test)\n\nlazy_models\n","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:53:32.925249Z","iopub.execute_input":"2023-03-13T15:53:32.925718Z","iopub.status.idle":"2023-03-13T15:53:55.280345Z","shell.execute_reply.started":"2023-03-13T15:53:32.925677Z","shell.execute_reply":"2023-03-13T15:53:55.279087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(random_state=0)\n\nlgbm_param_grid = [{\n    \"learning_rate\":[0.3, 0.1, 0.03],\n    \"n_estimators\":[100,50,200]\n}]\n\nlgbm_final_model = HyperParam_CV(lgbm, passengers_prepared, passengers_labels, lgbm_param_grid)    ","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:53:55.282245Z","iopub.execute_input":"2023-03-13T15:53:55.283031Z","iopub.status.idle":"2023-03-13T15:54:07.383242Z","shell.execute_reply.started":"2023-03-13T15:53:55.282985Z","shell.execute_reply":"2023-03-13T15:54:07.381985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End of training\n# Time to catch up the test set data predict on it","metadata":{}},{"cell_type":"markdown","source":"Now I need to bring my test set up to speed with my training set. Always satisfying after modulizing being able to do so in 2 line of code.","metadata":{}},{"cell_type":"code","source":"testset_passengers = earlyTransformations(testset_passengers, is_training_set=False)\ntestset_passengers_prepared = full_pipeline.fit_transform(testset_passengers)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:54:07.385246Z","iopub.execute_input":"2023-03-13T15:54:07.386212Z","iopub.status.idle":"2023-03-13T15:54:07.45947Z","shell.execute_reply.started":"2023-03-13T15:54:07.386156Z","shell.execute_reply":"2023-03-13T15:54:07.457675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the test set is up to speed with no errors, were ready to predict and submit.","metadata":{}},{"cell_type":"code","source":"test_preds = rfc_final_model.predict(testset_passengers_prepared)\ntest_preds","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:54:07.461224Z","iopub.execute_input":"2023-03-13T15:54:07.461751Z","iopub.status.idle":"2023-03-13T15:54:07.669225Z","shell.execute_reply.started":"2023-03-13T15:54:07.4617Z","shell.execute_reply":"2023-03-13T15:54:07.66811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['Transported'] = test_preds.astype(\"bool\")\nsubmission.to_csv(\"submission.csv\",index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-13T15:54:07.670959Z","iopub.execute_input":"2023-03-13T15:54:07.672232Z","iopub.status.idle":"2023-03-13T15:54:07.692434Z","shell.execute_reply.started":"2023-03-13T15:54:07.672176Z","shell.execute_reply":"2023-03-13T15:54:07.69111Z"},"trusted":true},"execution_count":null,"outputs":[]}]}